---
layout: course_page
title: Introduction to LLMs
published: true
module_slug: intro-to-llms
section_slug: modules_section
---

# Hallucinations and Why AI Sometimes Makes Stuff Up

Sometimes, LLMs produce responses that sound right—but are completely false. These are called *hallucinations*.

## Focus
- Why LLMs sometimes generate false information
- Model confidence vs. correctness
- Importance of critical thinking

## Analogy
Like a student who doesn’t know the answer, but tries to sound convincing anyway.

## Interactive Idea
Show examples of AI hallucinations and ask students to fact-check them.
